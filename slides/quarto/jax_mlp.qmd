---
title: "Deep Learning with Jax"
subtitle: "Feed-forward neural networks"
author: "Marco Forgione"
institute: "IDSIA USI-SUPSI, Lugano, Switzerland"
aspectratio: 169
theme: moloch
pdf-engine: lualatex
slide-level: 2
themeoptions:
  - block=fill
mathfont: NewCMMath-Regular.otf
include-in-header:
  text: |
    \usepackage[most]{tcolorbox}
    \usepackage{xcolor}
    \usepackage{pdfpages}
    \DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
    \newcommand{\NN}{\mathbb{N}}
    \newcommand{\RR}{\mathbb{R}}
    \newcommand{\indep}{\perp \!\!\! \perp}
format: 
  beamer:
    classoption: presentation
  beamer-handout:
    classoption: handout
echo: true
---

```{python}
#| echo: false
import jax
import jax.numpy as jnp
import jax.random as jr
import optax
import matplotlib.pyplot as plt
import matplotlib
import numpy as onp

onp.random.seed(42)

key = jr.key(42)
key_W1, key_b1, key_W2, key_b2 = jr.split(key, 4)

# Define your custom parameters as a dictionary
custom_params = {
    'figure.figsize': (4, 1.2),  # Width, Height in inches
    'font.size': 8,           # Default font size
    'axes.grid': True,         # Always show grid
}

# Update matplotlib's rcParams with your dictionary
plt.rcParams.update(custom_params)

```

## A non-linear dataset
Linear regression is not that exciting. Let's fit a neural net to a non-linear
dataset! 

```{python}
#| echo: false
def f(x):
    return 2*jnp.sin(3*x)  - 3*jnp.cos(7*x)

a = -1 # lower limit x
b = 1 # higher limit x
n_samples = 200 # data point
sigma_e = 0.1 # noise std
x_train = a + onp.random.rand(*(n_samples, 1))*(b - a);
x_train = x_train.reshape(-1, 1)
y_train = f(x_train)
```


```{python}
#| echo: false
plt.plot(x_train, y_train, "*k")
plt.xlabel("x")
plt.ylabel("y");
```

\pause
```{python}
x_train.shape, y_train.shape
```

## Feed-forward neural network
* A one-hidden-layer feed-forward neural network:
  $$y = W_2 \tanh (W_1 x + b_1) + b_2 $$

  with tunable weights $W_1, b_1, W_2, b_2$. 

\pause
* If we prefer a single parameter vector:
$$
p = \mathrm{vec}(W_1, b_1, W_2, b_2),\;\; W_1 \in \mathbb{R}^{n_h \times n_x}, b_1 \in \mathbb{R}^{n_h}, 
W_2 \in \mathbb{R}^{n_y \times n_h}, b_2 \in \mathbb{R}^{n_y}.
$$

\pause
* The feed-forward net is just a non-linear function:
$$y = f(p, x).$$

\pause
In our SISO problem $n_x=1, n_y=1$. We can choose $n_h$ (hyper-parameter).

## Feed-forward neural network
In code, it is convenient to organize parameters in a *dictionary*

:::: {.columns}
::: {.column width="60%"}

```{python}
def nn(p, x):
    z = jnp.tanh(p["W1"] @ x + p["b1"])
    y = p["W2"] @ z + p["b2"]
    return y
```
:::

::: {.column width="40%"}
$$ y  = W_2 \overbrace{\tanh (W_1 x + b_1)}^{=z} + b_2 $$
:::

::::
\pause
The parameter dictionary may be initialized like this:
```{python}
nx = 1; ny = 1; nh = 16
params = {
  "W1": jr.normal(key_W1, shape=(nh, nx)),
  "b1": jr.normal(key_b1, shape=(nh,)),
  "W2": jr.normal(key_W2, shape=(ny, nh)),
  "b2": jr.normal(key_b2, shape=(ny,)),
}
```
\pause
\vskip -.3em
Better initializations exist (Kaiming, Glorot,\dots). Key for big nets, omitted for simplicity.

## Feed-forward neural network implementation
* Let us apply the neural network to a data point:
  ```{python}
  nn(params, x_train[0])
  ```
  \pause
  The ``nn`` function only handles a single data point by construction. 
  ```{python}
  # nn(p_hat, x_train) # x_train: (N, n_x). It would not work!
  ```
  \pause

* To process a *batch*, we must *vectorize* the ``nn`` function wrt its second argument.

  ```{python}
  # do nothing for first arg, add batch axis 0 for 2nd arg
  batched_nn = jax.vmap(nn, in_axes=(None, 0))
  ```

  ```{python}
  y_hat = batched_nn(params, x_train); 
  y_hat.shape
  ```

## Setting up the loss
 * Define the loss as a function
```{python}
  def loss_fn(p, y, x):
    y_hat = batched_nn(p, x)
    loss = jnp.mean((y - y_hat) ** 2)
    return loss
```

\pause
* Define function that return both loss and its gradient
```{python}
loss_grad_fn = jax.value_and_grad(loss_fn, 0)
loss_grad_fn = jax.jit(loss_grad_fn) # compile it!
```

\pause

```{python}
loss_grad_fn(params, y_train, x_train)
```

## Fitting the neural net
```{python}
#| echo: false 
params_init = params
```
The *boilerplate* training code. Use `optax` instead of gradient descent from scratch\dots


```{python}
optimizer = optax.adam(learning_rate=1e-2) # optax.{sgd, adam, ...}
opt_state = optimizer.init(params)
```
```{python}
# Training loop
LOSS = []
for iter in range(1000):
    loss_val, grads = loss_grad_fn(params, y_train, x_train)

    updates, opt_state = optimizer.update(grads, opt_state)
    params = optax.apply_updates(params, updates)
    
    LOSS.append(loss_val)
```

## Results
:::: {.columns}
:::{.column width="50%"}
```{python}
#| echo: false
plt.figure(figsize=(3.5, 2))
plt.title("Loss vs. Iteration")
plt.plot(LOSS)
plt.xlabel("Iteration (-)")
plt.ylabel("Loss");
```
:::

:::{.column width="50%"}
```{python}
#| echo: false
x_train_srt = jnp.sort(x_train, axis=0)
plt.figure(figsize=(3.5, 2))
plt.title("Model fit")
plt.plot(x_train, y_train, "k*", label="y")
plt.plot(x_train_srt, batched_nn(params, x_train_srt), "g", label="$f(p^{1000}, x)$")
plt.plot(x_train_srt, batched_nn(params_init, x_train_srt), "b", label="$f(p^{1}, x)$")
plt.xlabel("x")
plt.ylabel("y")
plt.legend();

```
:::

::::

\pause
* Good fit without knowing the model structure
\pause
* Can be improved with hyperparameter tuning, better optimizer and initialization\dots
\pause
* Extension to MIMO is trivial - just change $n_x, n_y$
\pause 
* Extension to dynamical systems tomorrow


## Exercises

* Try out different hyper-parameters (e.g., hidden layers $n_h$, non-linearity function)
* Try out different initialization and optimization settings
* Define and train a neural network with 2 hidden layers
* Replicate the 2D example in the introductory slides