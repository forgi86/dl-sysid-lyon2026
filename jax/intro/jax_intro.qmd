---
title: "Deep Learning with Jax"
subtitle: "Introduction"
author: "Marco Forgione"
institute: "IDSIA USI-SUPSI, Lugano, Switzerland"
aspectratio: 169
theme: moloch
pdf-engine: lualatex
slide-level: 2
themeoptions:
  - block=fill
mathfont: NewCMMath-Regular.otf
include-in-header:
  text: |
    \usepackage[most]{tcolorbox}
    \usepackage{xcolor}
    \usepackage{pdfpages}
    \DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
    \newcommand{\NN}{\mathbb{N}}
    \newcommand{\RR}{\mathbb{R}}
    \newcommand{\indep}{\perp \!\!\! \perp}
format: 
  beamer:
    classoption: presentation
  beamer-handout:
    classoption: handout
echo: true
---


## Getting started
Jax has an interface that behaves like numpy (or MATLAB). Let's compute:

$$
\begin{bmatrix}
1 & 1
\end{bmatrix}
+
\begin{bmatrix}
1 & 2
\end{bmatrix} = 
\begin{bmatrix}
2 & 3
\end{bmatrix}
$$

\pause
\vskip 1em 

:::: {.columns}

::: {.column width="50%"}
MATLAB
```matlab
a = ones(1, 2);
b = [1.0, 2.0];
a + b
```
\texttt{[2    3]}
:::

::: {.column width="50%"}
Jax

```{python}
#| echo: false
import jax
import jax.numpy as jnp
import jax.random as jr
import matplotlib.pyplot as plt
import matplotlib
import numpy as onp

onp.random.seed(42)

key = jr.key(42)
key_e, key_p = jr.split(key, 2)

# Define your custom parameters as a dictionary
custom_params = {
    'figure.figsize': (4, 1.2),  # Width, Height in inches
    'font.size': 8,           # Default font size
    'axes.grid': True,         # Always show grid
}

# Update matplotlib's rcParams with your dictionary
plt.rcParams.update(custom_params)

```

```{python}
a = jnp.ones((2,))
b = jnp.array([1.0, 2.0])
a + b
```
:::

::::

\pause

\vskip 1em
* Jax is slightly more verbose. The price of a general-purpose language like Python.
\pause
* Jax allows you define a generic 1D vector (neither a row or a column).
\pause
* A few more subtle differences...

## Function definition

* Define the function: 
$$ f(p, x) : \mathbb{R}^2 \times \mathbb{R} \mapsto \mathbb{R} = p_1 x + p_2 $$

\pause
:::: {.columns}

::: {.column width="40%"}
MATLAB
```matlab
function y = f(p, x)
    y = p(1) * x + p(2);
end
```
:::

::: {.column width="60%"}
Jax
```{python}
def f(p, x):
    y = p[0] * x  + p[1]
    return y
```
:::

::::

\vskip .5em
We shall interpret $f$ as the model structure, $p$ as the parameter, $x$ as the input.

\pause
* Apply $f$ with "true" $p^o = [1.0\;\; 2.0]$ to an input data point $x = 0.5$

\pause
:::: {.columns}
::: {.column width="40%"}
```matlab
p_o = [1.0, 2.0];
x = 0.5;
f(p_o, x)
```
\texttt{2.5000}
:::

::: {.column width="60%"}
```{python}
p_o = jnp.array([1.0, 2.0])
x = jnp.array(0.5)
y = f(p_o, x); y
```
:::
::::

## Linear regression dataset
* Apply $f$ with $p^o$ to $N=100$ linearly spaced points in $[-2\;\; 2]$, add noise.
```{python}
N = 100
x = jnp.linspace(-2, 2, N)
y = f(p_o, x) + jr.normal(key_e, (N,)) * 0.1
```


```{python}
#| echo: false
plt.plot(x, y, "k*", label="y")
plt.title("Training dataset")
plt.xlabel("x")
plt.ylabel("y");
```

\pause
Function `f` works correctly both with scalar and vector input `x`.


## Loss definition
The Mean Squared Error (MSE) loss is:
$$ \mathcal{L}(p, y, x) : \mathbb{R}^{n_p} \times \mathbb{R}^N \times \mathbb{R}^N \mapsto \mathbb{R}
= \frac{1}{N}\sum_{i=1}^{N} \big(y_i - f(p, y_i, x_i)\big)^2
$$
with $n_p=2$ parameters.

\pause

We exploit that `f` can process a vector input `x`:
```{python}
def loss_fn(p, y, x):
    ym = f(p, x) # works with vector x
    loss = jnp.mean((y - ym) ** 2)
    return loss
```

\pause
```{python}
p_hat = jr.normal(key_p, shape=(2,))
loss_fn(p_hat, y, x)
```


## Automatic differentiation in Jax
* For gradient-based optimization, we need the gradient:
$$
\nabla_{1} \mathcal{L}(p, y, x): \mathbb{R}^{n_p} \times \mathbb{R}^N \times \mathbb{R}^N \mapsto \mathbb{R}^{n_p},
$$
i.e. the derivative of $\mathcal{L}$ with respect to its first argument: $p$.

\pause

* The main point of Jax (PyTorch, ...) is *automatic differentiation*. Efficient & effortless numerical evaluation of derivatives of interest!
\pause
```{python}
grad_fn = jax.grad(loss_fn, 0) # gradient wrt 1st agrument
```

\pause
The function ``grad_fn`` can be called on arbitraty arguments:
```{python}
grad_fn(p_hat, y, x)
```


## Fitting a model with Jax
With automatic differentiation, setting up gradient descent is a piece of cake.
```{python}
#| echo: false 
p_init = p_hat
```

:::: {.columns}
::: {.column width="52%"}
```{python}
#| eval: false
p_hat = jr.normal(key_p, shape=(2,))
lr = 1e-2 # learning rate

for i in range(200):
    g = grad_fn(p_hat, y, x)
    p_hat = p_hat - lr * g 
```

```{python}
#| eval: true
#| echo: false
p_hat = jr.normal(key_p, shape=(2,))
lr = 1e-2 # learning rate
LOSS = []
for i in range(200):
    LOSS.append(loss_fn(p_hat, y, x))
    g = grad_fn(p_hat, y, x)
    p_hat = p_hat - lr * g 
```
:::


:::  {.column width="47%"}
\vskip 3em
$$ p^{k+1} = p^{k} - \lambda \nabla_1 \mathcal{L}(p^{k}, y, x) $$
:::
::::

\pause


```{python}
#| echo: false
fig, ax = plt.subplots(1, 2, figsize=(6, 1.7))
ax[0].set_title("Loss vs. Iterations")
ax[0].plot(LOSS)
ax[1].set_title("Model fit")
ax[1].plot(x, y, "k*", label="y")
ax[1].plot(x, f(p_hat, x), "g", label="$f(p^{200}, x)$")
ax[1].plot(x, f(p_init, x), "b", label="$f(p^{1}, x)$")
ax[1].legend()
plt.tight_layout()
```

