{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d94aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Non-linear regression with feedforward networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48032199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c7e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jr.key(42)\n",
    "key_x, key_W1, key_b1, key_W2, key_b2 = jr.split(key, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a48036",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "\n",
    "Synthetic data from the function:\n",
    "\n",
    "$$y = 2 \\sin(3x) - 3 \\cos(7x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d8c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 2*jnp.sin(3*x)  - 3*jnp.cos(7*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20727e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = -1 # lower limit x\n",
    "b = 1 # higher limit x\n",
    "n_samples = 200 # data point\n",
    "sigma_e = 0.1 # noise std\n",
    "x_train = a + jr.uniform(key_x, shape=(n_samples, 1))*(b - a);\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "y_train = f(x_train)\n",
    "\n",
    "plt.plot(x_train, y_train, \"*k\")\n",
    "plt.title(\"Training data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97711f",
   "metadata": {},
   "source": [
    "Always good to check data types and shapes. Saves like 80% of debugging time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a75f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c434cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.dtype, y_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5176a7c",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "Define the feedforward neural network with one hidden layer:\n",
    "\n",
    "  $$\\hat y = W_2 \\tanh (W_1 x + b_1) + b_2 $$\n",
    "\n",
    "The parameters to be tuned are:\n",
    "\n",
    "  $$p = \\mathrm{vec}(W_1, b_1, W_2, b_2)$$\n",
    "  $$ W_1 \\in \\mathbb{R}^{n_h \\times n_x}, b_1 \\in \\mathbb{R}^{n_h}, \n",
    "W_2 \\in \\mathbb{R}^{n_y \\times n_h}, b_2 \\in \\mathbb{R}^{n_y}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ff5e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all parameters and organize them in a dictionary\n",
    "\n",
    "nx = 1; ny = 1; nh = 16\n",
    "p_hat = {\n",
    "  \"W1\": jr.normal(key_W1, shape=(nh, nx)),\n",
    "  \"b1\": jr.normal(key_b1, shape=(nh,)),\n",
    "  \"W2\": jr.normal(key_W2, shape=(ny, nh)),\n",
    "  \"b2\": jr.normal(key_b2, shape=(ny,)),\n",
    "}\n",
    "\n",
    "p_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90642db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network as a function of parameters and inputs\n",
    "\n",
    "def nn(p, x):\n",
    "    z = jnp.tanh(p[\"W1\"] @ x + p[\"b1\"])\n",
    "    y = p[\"W2\"] @ z + p[\"b2\"]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5304d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the neural network with initial parameters and a sample input\n",
    "\n",
    "nn(p_hat, x_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9426c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This fails because of shape mismatch, we need to vectorize the nn function\n",
    "# nn(p_hat, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa269c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do nothing for first arg, expect a batch axis at the left (0th axis) for second arg\n",
    "batched_nn = jax.vmap(nn, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b5f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The batched output also has a batch axis at the left (0th axis). Just what we want!\n",
    "y = batched_nn(p_hat, x_train)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d301bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that it is correct, if you don't believe!\n",
    "nn(p_hat, x_train[10]), y[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a17647c",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "From now on, it's more or less like what we did for linear regression! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b66484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(p, y, x):\n",
    "    ym = batched_nn(p, x)\n",
    "    loss = jnp.mean((y - ym) ** 2)\n",
    "    return loss\n",
    "\n",
    "# the function loss_grad_fn will return both loss and gradient of the loss\n",
    "loss_grad_fn = jax.value_and_grad(loss_fn, 0)\n",
    "\n",
    "# Important performance trick: just-in-time compilation for this compute-intensive part!\n",
    "loss_grad_fn = jax.jit(loss_grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf01f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_init = p_hat # save it just for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5bf857",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2 # learning rate\n",
    "LOSS = []\n",
    "for i in range(10_000):\n",
    "    l, g = loss_grad_fn(p_hat, y_train, x_train)\n",
    "    p_hat = jax.tree.map(lambda x, y: x - lr*y, p_hat, g)\n",
    "    LOSS.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21563f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_srt = jnp.sort(x_train, axis=0)\n",
    "plt.figure()\n",
    "plt.title(\"Model fit\")\n",
    "plt.plot(x_train, y_train, \"k*\", label=\"y\")\n",
    "plt.plot(x_train_srt, batched_nn(p_hat, x_train_srt), \"g\", label=\"$f(p^{200}, x)$\")\n",
    "plt.plot(x_train_srt, batched_nn(p_init, x_train_srt), \"b\", label=\"$f(p^{1}, x)$\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c857056",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Loss vs. Iteration\")\n",
    "plt.plot(LOSS)\n",
    "plt.xlabel(\"Iteration (-)\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
